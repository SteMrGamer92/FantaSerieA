name: Scraper Serie A

on:
  schedule:
    # ========== VENERDÃŒ ==========
    # Partita 20:45 IT
    - cron: '30 18 * * 5'   # 20:30 IT (15 min prima)
    - cron: '45 20 * * 5'   # 22:45 IT (2h dopo inizio)
    
    # ========== SABATO ==========
    # Partita 15:00 IT
    - cron: '45 12 * * 6'   # 14:45 IT (15 min prima)
    - cron: '0 15 * * 6'    # 17:00 IT (2h dopo inizio)
    
    # Partita 18:00 IT
    - cron: '45 15 * * 6'   # 17:45 IT (15 min prima)
    - cron: '0 18 * * 6'    # 20:00 IT (2h dopo inizio)
    
    # Partita 20:45 IT
    - cron: '30 18 * * 6'   # 20:30 IT (15 min prima)
    - cron: '45 20 * * 6'   # 22:45 IT (2h dopo inizio)
    
    # ========== DOMENICA ==========
    # Partita 12:30 IT
    - cron: '15 10 * * 0'   # 12:15 IT (15 min prima)
    - cron: '30 12 * * 0'   # 14:30 IT (2h dopo inizio)
    
    # Partita 15:00 IT
    - cron: '45 12 * * 0'   # 14:45 IT (15 min prima)
    - cron: '0 15 * * 0'    # 17:00 IT (2h dopo inizio)
    
    # Partita 18:00 IT
    - cron: '45 15 * * 0'   # 17:45 IT (15 min prima)
    - cron: '0 18 * * 0'    # 20:00 IT (2h dopo inizio)
    
    # Partita 20:45 IT
    - cron: '30 18 * * 0'   # 20:30 IT (15 min prima)
    - cron: '45 20 * * 0'   # 22:45 IT (2h dopo inizio)

    # ========== LUNEDI ==========
    # Partita 18:00 IT
    - cron: '45 15 * * 1'   # 17:45 IT (15 min prima)
    - cron: '0 18 * * 1'    # 20:00 IT (2h dopo inizio)
    
    # Partita 20:45 IT
    - cron: '30 18 * * 1'   # 20:30 IT (15 min prima)
    - cron: '45 20 * * 1'   # 22:45 IT (2h dopo inizio)
  
  workflow_dispatch:  # Esecuzione manuale

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v3
      
      - name: ðŸ Setup Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: ðŸ“¦ Install dependencies
        run: |
          pip install playwright lxml pytz supabase
          playwright install chromium --with-deps
      
      - name: ðŸš€ Run scraper
        run: python scrap.py
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

      # âœ… SALVA HTML COME ARTIFACT
      - name: ðŸ“¤ Upload HTML debug files
        if: always()  # Esegui anche se scraper fallisce
        uses: actions/upload-artifact@v3
        with:
          name: html-debug-files
          path: /tmp/match_*.html
          retention-days: 1  # Conserva per 7 giorni
          
      - name: ðŸ“Š Log risultato
        if: always()
        run: |
          echo "âœ… Scraping completato"
          ls -lh /tmp/match_*.html 2>/dev/null || echo "Nessun file HTML generato"
